{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um Web App de Análise de Sentimento\n",
    "## Usando PyTorch e SageMaker\n",
    "\n",
    "_Esse tutorial foi feito com base no projeto final do Nanodegree em Deep Learning da Udacity_\n",
    "\n",
    "---\n",
    "\n",
    "Agora que vocês já conhecem um pouco de AWS e de Sagemaker, iremos construir um projeto completo (end to end) com o intuito de desenvolver um web app onde o usuário insira um texto de avaliação de filme e saiba se essa avaliação é positiva ou negativa.\n",
    "\n",
    "## Linhas Gerais\n",
    "\n",
    "\n",
    "Esboço geral para projetos SageMaker usando uma instância de notebook.\n",
    "\n",
    "1. Baixe ou recupere os dados.\n",
    "2. Processe / prepare os dados.\n",
    "3. Faça upload dos dados processados para S3.\n",
    "4. Treine um modelo escolhido.\n",
    "5. Teste o modelo treinado (normalmente usando um trabalho de transformação em lote).\n",
    "6. Implante o modelo treinado.\n",
    "7. Use o modelo implantado.\n",
    "\n",
    "Para este projeto, seguiremos todas as etapas gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==1.72.0\n",
      "  Downloading sagemaker-1.72.0.tar.gz (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 4.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.20.4)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.19.1)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.5.3)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Collecting smdebug-rulesconfig==0.1.4\n",
      "  Downloading smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (4.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (21.2)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.23.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.24.0,>=1.23.4->boto3>=1.14.12->sagemaker==1.72.0) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.24.0,>=1.23.4->boto3>=1.14.12->sagemaker==1.72.0) (1.26.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker==1.72.0) (1.16.0)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-1.72.0-py2.py3-none-any.whl size=388346 sha256=3df83142c25b5950de80faee5f73b95a86ecb9d029894e13cc37b24e126e48d9\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/58/70/85faf4437568bfaa4c419937569ba1fe54d44c5db42406bbd7\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: smdebug-rulesconfig, sagemaker\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 1.0.1\n",
      "    Uninstalling smdebug-rulesconfig-1.0.1:\n",
      "      Successfully uninstalled smdebug-rulesconfig-1.0.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.68.0\n",
      "    Uninstalling sagemaker-2.68.0:\n",
      "      Successfully uninstalled sagemaker-2.68.0\n",
      "Successfully installed sagemaker-2.60.0 smdebug-rulesconfig-0.1.4\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Make sure that we use SageMaker 1.x\n",
    "!pip install sagemaker==1.72.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 1: Baixando os dados\n",
    "\n",
    "Para o nosso projeto, usaremos uma base bastante conhecida chamada [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data’: File exists\n",
      "--2021-12-01 04:31:48--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  34.4MB/s    in 2.3s    \n",
      "\n",
      "2021-12-01 04:31:50 (34.4 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 2: Preparando e processando os dados\n",
    "\n",
    "Processaremos os nossos dados a fim de deixá-los em um formato mais fácil para realizar o treinamento do modelo. Primeiramente, iremos unir as avaliação positivas e negativas em uma mesma estrutura de dados, uma vez que os dados vem em arquivos separados. Após isso iremos separá-los em treino e teste, garantindo que eles estejam misturados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que lemos os dados brutos de treinamento e teste do conjunto de dados baixado, combinaremos as avaliações positivas e negativas e embaralharemos os registros resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos dar uma olhadinha nos nossos dados!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This, despite not being the original - it began life as a play in Central Europe - has weathered the several incarnations that followed (MGM's own remake with period songs In The Good Old Summertime, the Broadway show She Loves Me, even the excellent theatre revival in Paris a couple of years ago) and remains the definitive version and the one they all have to beat. Several previous commenters have identified the contributing factors that make it so successful and memorable not least being the prevailing fashion in 30s and 40s Hollywood for lavishing attention and detail on ensemble playing rather than just two leads as so often happens today - try, for example, removing Ugarte, Ferrari, Renault etc from Casablanca and yes you'd still have Rick and Ilsa and Viktor Lazslo but they'd just be frosting without the rich cake mixture below. Jimmy Stewart and Maggie Sullavan WERE both ideal and irreplaceable leads but how much brighter they shine when their performances are reflected in those of Frank Morgan, Felix Bressart, Joseph Schildkraut and Andy Hardy's Sara Haden and this is BEFORE we factor in that Lubitsch 'touch'. Okay, maybe they WERE a tad more naive, innocent even, in that Jurassic Age but how many genuine film lovers, sated with scatology, screwing and in-your-face sex, turn back to those days of Stories, Style, Slickness and Skill and wallow in great movies like this one. By far the best thing about this technological age is not CSI but DVD that can at one and the same time make these classics available to nostalgics and show the Matrix freaks how the big boys used to do it.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_X[100])\n",
    "print(train_y[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como primeiro passo de pré processamento, iremos limpar as tags HTML que podem aparecer nas avaliações e, após isso, iremos _tokenizar_ nossos dados para que palavras como *entertained* e *entertaining* sejam consideradas iguais no nosso modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def review_to_words(review):\n",
    "    stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `review_to_words` utiliza a biblioteca `BeautifulSoup` para remover as tags HTML e usa a biblioteca `nltk` para tokenizar as avaliações e remover as stopwords.  \n",
    "Abaixo, podemos ver o output dessa função aplicado a uma avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['despit',\n",
       " 'origin',\n",
       " 'began',\n",
       " 'life',\n",
       " 'play',\n",
       " 'central',\n",
       " 'europ',\n",
       " 'weather',\n",
       " 'sever',\n",
       " 'incarn',\n",
       " 'follow',\n",
       " 'mgm',\n",
       " 'remak',\n",
       " 'period',\n",
       " 'song',\n",
       " 'good',\n",
       " 'old',\n",
       " 'summertim',\n",
       " 'broadway',\n",
       " 'show',\n",
       " 'love',\n",
       " 'even',\n",
       " 'excel',\n",
       " 'theatr',\n",
       " 'reviv',\n",
       " 'pari',\n",
       " 'coupl',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'remain',\n",
       " 'definit',\n",
       " 'version',\n",
       " 'one',\n",
       " 'beat',\n",
       " 'sever',\n",
       " 'previou',\n",
       " 'comment',\n",
       " 'identifi',\n",
       " 'contribut',\n",
       " 'factor',\n",
       " 'make',\n",
       " 'success',\n",
       " 'memor',\n",
       " 'least',\n",
       " 'prevail',\n",
       " 'fashion',\n",
       " '30',\n",
       " '40',\n",
       " 'hollywood',\n",
       " 'lavish',\n",
       " 'attent',\n",
       " 'detail',\n",
       " 'ensembl',\n",
       " 'play',\n",
       " 'rather',\n",
       " 'two',\n",
       " 'lead',\n",
       " 'often',\n",
       " 'happen',\n",
       " 'today',\n",
       " 'tri',\n",
       " 'exampl',\n",
       " 'remov',\n",
       " 'ugart',\n",
       " 'ferrari',\n",
       " 'renault',\n",
       " 'etc',\n",
       " 'casablanca',\n",
       " 'ye',\n",
       " 'd',\n",
       " 'still',\n",
       " 'rick',\n",
       " 'ilsa',\n",
       " 'viktor',\n",
       " 'lazslo',\n",
       " 'd',\n",
       " 'frost',\n",
       " 'without',\n",
       " 'rich',\n",
       " 'cake',\n",
       " 'mixtur',\n",
       " 'jimmi',\n",
       " 'stewart',\n",
       " 'maggi',\n",
       " 'sullavan',\n",
       " 'ideal',\n",
       " 'irreplac',\n",
       " 'lead',\n",
       " 'much',\n",
       " 'brighter',\n",
       " 'shine',\n",
       " 'perform',\n",
       " 'reflect',\n",
       " 'frank',\n",
       " 'morgan',\n",
       " 'felix',\n",
       " 'bressart',\n",
       " 'joseph',\n",
       " 'schildkraut',\n",
       " 'andi',\n",
       " 'hardi',\n",
       " 'sara',\n",
       " 'haden',\n",
       " 'factor',\n",
       " 'lubitsch',\n",
       " 'touch',\n",
       " 'okay',\n",
       " 'mayb',\n",
       " 'tad',\n",
       " 'naiv',\n",
       " 'innoc',\n",
       " 'even',\n",
       " 'jurass',\n",
       " 'age',\n",
       " 'mani',\n",
       " 'genuin',\n",
       " 'film',\n",
       " 'lover',\n",
       " 'sate',\n",
       " 'scatolog',\n",
       " 'screw',\n",
       " 'face',\n",
       " 'sex',\n",
       " 'turn',\n",
       " 'back',\n",
       " 'day',\n",
       " 'stori',\n",
       " 'style',\n",
       " 'slick',\n",
       " 'skill',\n",
       " 'wallow',\n",
       " 'great',\n",
       " 'movi',\n",
       " 'like',\n",
       " 'one',\n",
       " 'far',\n",
       " 'best',\n",
       " 'thing',\n",
       " 'technolog',\n",
       " 'age',\n",
       " 'csi',\n",
       " 'dvd',\n",
       " 'one',\n",
       " 'time',\n",
       " 'make',\n",
       " 'classic',\n",
       " 'avail',\n",
       " 'nostalg',\n",
       " 'show',\n",
       " 'matrix',\n",
       " 'freak',\n",
       " 'big',\n",
       " 'boy',\n",
       " 'use']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words(train_X[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já a função abaixo, `preprocess_data`, aplica a função `review_to_words` para cada uma das avaliações dos datasets de treino e teste. Além disso, ela também faz o cache dos dados, para que, caso algo aconteça, você possa voltar o pré processamento de onde parou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformando os dados\n",
    "\n",
    "Agora, nós iremos construir uma representação dos nossos dados muito similar a representação conhecida como bag-of-words. Para a rede neural recorrente que iremos usar, nós iremos relizar a transformação dos dados da seguinte forma:\n",
    "\n",
    "1. Transformar cada palavra em um número inteiro;\n",
    "2. Definir um tamanho para nosso vocabulário, ou seja, iremos remover palavras que aparecem pouco (para essas palavras atribuiremos o mesmo número inteiro (1));\n",
    "3. Como estamos usando uma RNN, defineros um tamanho para nossas sequência, ou seja, truncaremos aquelas que forem maiores e iremos inserir um caractér (0) para quando a avaliação for menor do que o tamanho definido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começaremos construindo uma função que nos retorna um dicionário de tamanho especifíco e com as palavras que mais aparecem. Não podemos esquecer de reservar o índice 0 e 1 para os caractéres vazio e pouco frequente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    for review in data:\n",
    "        for word in review:\n",
    "            if word_count.get(word) is None:\n",
    "                word_count[word] = 1\n",
    "            else:\n",
    "                word_count[word] += 1\n",
    "    \n",
    "    sorted_words = [k for k,v in sorted(word_count.items(), key=lambda item: item[1], reverse=True)]\n",
    "    \n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dict(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando nosso dicionário\n",
    "\n",
    "Mais para frente, quando tivermos nosso modelo, teremos que usar nosso dicionário para realizar as predições. Sendo assim, precisamos salvá-lo para usar no futuro!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/pytorch' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformando as avaliações\n",
    "\n",
    "Agora, é hora de convertermos nossas avaliações de treino e teste para a sequência de número inteiros de tamanho fixo que entrará na nossa rede neural!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
    "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apenas para validar se nossa função está fazendo tudo corretamente, vamos verificar um dado da nossa base de treino e ver seu tamanho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4340    2   25   24   69 2295  128   31   65  220   44  176  125  206\n",
      "   17  118  504  123   11   16   49   11  267  843  694  843   18    7\n",
      " 1653    2  862    1  205  176   20 2494 2794 1099 1183   38   95 3932\n",
      " 2433  761 2494 2794 1670 3932  279  308    1  161    1    1  135  851\n",
      " 2028   17  233  208   74   54  213 2295  128    5  220  145  296    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to examine one of the processed reviews to make sure everything is working as intended.\n",
    "print(train_X[100])\n",
    "print(len(train_X[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 3: Subindo nossos dados de treino para o S3\n",
    "\n",
    "Nós iremos precisar subir nossos dados de treino para o S3 para que possamos acessá-lo durante o treinamento.\n",
    "\n",
    "### Salvando os dados de treino localmente\n",
    "\n",
    "Antes de subir para o S3, iremos salvar nossos dados localmente. É muito importante saber a estrutura dos dados que vamos salvar, para que possamos utilizar de forma correta. No nosso caso, as linhas do nosso dataset irão ter a forma (colunas): `label`, `length`, `review[500]`, onde `review[500]`é a sequência com 500 números inteiros que geramos acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subindo para o S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/sentiment_rnn'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS:** A célula acima sobe todos os arquivos contidos no nosso diretório para o S3. Isso inclui o `word_dict.pkl`, que usaremos na hora de realizar uma nova predição e garantir que o pré processamento dos novos dados seja o mesmo dos dados de treino. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 4: Construindo e treinando nosso modelo em Pytorch\n",
    "\n",
    "\n",
    "Em particular, um modelo compreende três objetos\n",
    "\n",
    " - Artefatos de modelo,\n",
    " - Código de Treinamento e\n",
    " - Código de inferência,\n",
    " \n",
    "cada um dos quais interage um com o outro. Implementaremos nossa própria rede neural no PyTorch junto com um script de treinamento. Para os fins deste projeto, fornecemos o objeto de modelo necessário no arquivo `model.py`, dentro da pasta `train`. Você pode ver a implementação fornecida executando a célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTMClassifier\u001b[39;49;00m(nn.Module):\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    This is the simple RNN model we will be using to perform Sentiment Analysis.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dim, hidden_dim, vocab_size):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Initialize the model by settingg up the various layers.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(LSTMClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=\u001b[34m0\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=\u001b[34m1\u001b[39;49;00m)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()\r\n",
      "        \r\n",
      "        \u001b[36mself\u001b[39;49;00m.word_dict = \u001b[34mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Perform a forward pass of our model on some input.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        x = x.t()\r\n",
      "        lengths = x[\u001b[34m0\u001b[39;49;00m,:]\r\n",
      "        reviews = x[\u001b[34m1\u001b[39;49;00m:,:]\r\n",
      "        embeds = \u001b[36mself\u001b[39;49;00m.embedding(reviews)\r\n",
      "        lstm_out, _ = \u001b[36mself\u001b[39;49;00m.lstm(embeds)\r\n",
      "        out = \u001b[36mself\u001b[39;49;00m.dense(lstm_out)\r\n",
      "        out = out[lengths - \u001b[34m1\u001b[39;49;00m, \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(lengths))]\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(out.squeeze())\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize train/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A conclusão importante da implementação fornecida é que existem três parâmetros que podemos desejar ajustar para melhorar o desempenho de nosso modelo. Estas são a dimensão de embedding, a dimensão oculta e o tamanho do vocabulário. Provavelmente, desejaremos tornar esses parâmetros configuráveis no script de treinamento para que, se desejarmos modificá-los, não precisemos modificar o próprio script. Para começar, escreveremos parte do código de treinamento no notebook para que possamos diagnosticar mais facilmente quaisquer problemas que surjam.\n",
    "\n",
    "Primeiro, carregaremos uma pequena parte do conjunto de dados de treinamento para usar como amostra. Seria muito demorado tentar treinar o modelo completamente no notebook, pois não temos acesso a uma gpu e a instância de computação que estamos usando não é particularmente poderosa. No entanto, podemos trabalhar com alguns dados para ter uma ideia de como nosso script de treinamento está se comportando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Read in only the first 250 rows\n",
    "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Escrevendo o método de treinamento\n",
    "\n",
    "Em seguida, precisamos escrever o código de treinamento. Isso deve ser muito semelhante aos métodos de treinamento que escrevemos antes para treinar modelos em PyTorch. Vamos deixar todos os aspectos difíceis, como salvar / carregar o modelo e carregar os parâmetros, para um pouco mais tarde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:         \n",
    "            batch_X, batch_y = batch\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(batch_X)\n",
    "            loss = loss_fn(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supondo que temos o método de treinamento acima, testaremos se ele está funcionando escrevendo um pouco de código no notebook que executa nosso método de treinamento no pequeno conjunto de treinamento de amostra que carregamos anteriormente. A razão para fazer isso no notebook é para que tenhamos a oportunidade de corrigir quaisquer erros que surjam no início, quando são mais fáceis de diagnosticar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, BCELoss: 0.6916054129600525\n",
      "Epoch: 2, BCELoss: 0.6835224390029907\n",
      "Epoch: 3, BCELoss: 0.6764762163162231\n",
      "Epoch: 4, BCELoss: 0.6685465693473815\n",
      "Epoch: 5, BCELoss: 0.658632218837738\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from train.model import LSTMClassifier\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(32, 100, 5000).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "train(model, train_sample_dl, 5, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir um modelo PyTorch usando o SageMaker, devemos fornecer ao SageMaker um script de treinamento. Podemos opcionalmente incluir um diretório que será copiado para o contêiner e a partir do qual nosso código de treinamento será executado. Quando o contêiner de treinamento é executado, ele verifica o diretório carregado (se houver) para um arquivo `requirements.txt` e instala todas as bibliotecas Python necessárias, após o qual o script de treinamento será executado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando o modelo\n",
    "\n",
    "Quando um modelo PyTorch é construído no SageMaker, um ponto de entrada deve ser especificado. Este é o arquivo Python que será executado quando o modelo for treinado. Dentro do diretório `train` está um arquivo chamado` train.py` que é fornecido e que contém o código necessário para treinar nosso modelo. \n",
    "\n",
    "A maneira como o SageMaker passa hiperparâmetros para o script de treinamento é por meio de argumentos. Esses argumentos podem ser analisados e usados no script de treinamento. Para ver como isso é feito, dê uma olhada no arquivo `train / train.py` fornecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.c5.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 3,\n",
    "                        'hidden_dim': 200,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-01 04:32:13 Starting - Starting the training job...\n",
      "2021-12-01 04:32:14 Starting - Launching requested ML instances...\n",
      "2021-12-01 04:33:03 Starting - Preparing the instances for training.........\n",
      "2021-12-01 04:34:41 Downloading - Downloading input data\n",
      "2021-12-01 04:34:41 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-12-01 04:34:41,999 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-12-01 04:34:42,001 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-01 04:34:42,016 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-12-01 04:34:48,234 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-12-01 04:34:48,413 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-12-01 04:34:48,414 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-12-01 04:34:48,414 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-12-01 04:34:48,414 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/69/bf/f0f194d3379d3f3347478bd267f754fc68c11cbf2fe302a6ab69447b1417/beautifulsoup4-4.10.0-py3-none-any.whl (97kB)\u001b[0m\n",
      "\u001b[34mCollecting html5lib (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl (112kB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/e3/d9f046b5d1c94a3aeab15f1f867aa414f8ee9d196fae6865f1d6a0ee1a0b/pytz-2021.3-py2.py3-none-any.whl (503kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mCollecting regex (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/97/cd/93ad08b2f97ec95da0bd860380ce0ac7481eaccc760356ee11eda369c048/regex-2021.11.10.tar.gz (702kB)\u001b[0m\n",
      "\u001b[34mCollecting joblib (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/63/f3/b7a1b8e40fd1bd049a34566eb353527bb9b8e9b98f8b6cf803bb64d8ce95/tqdm-4.62.3-py2.py3-none-any.whl (76kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2 (from beautifulsoup4->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/02/fb/1c65691a9aeb7bd6ac2aa505b84cb8b49ac29c976411c6ab3659425e045f/soupsieve-2.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.5/dist-packages (from html5lib->-r requirements.txt (line 5)) (1.11.0)\u001b[0m\n",
      "\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train, regex\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xaea1k_1/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "  Running setup.py bdist_wheel for regex: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/cd/5d/f5/8d8fa6ffd0251556598eb71da0c5c0374bca976273ade005ed\u001b[0m\n",
      "\u001b[34mSuccessfully built train regex\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, numpy, pandas, regex, joblib, tqdm, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\u001b[0m\n",
      "\u001b[34m  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed beautifulsoup4-4.10.0 html5lib-1.1 joblib-0.14.1 nltk-3.6.2 numpy-1.18.5 pandas-0.24.2 pytz-2021.3 regex-2021.11.10 soupsieve-2.1 tqdm-4.62.3 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.3.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-12-01 04:35:06,416 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-12-01 04:35:06,427 sagemaker-containers INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"TrainingInputMode\": \"File\"\n",
      "        }\n",
      "    },\n",
      "    \"num_gpus\": 0,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\"\n",
      "    },\n",
      "    \"num_cpus\": 4,\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 3,\n",
      "        \"hidden_dim\": 200\n",
      "    },\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"log_level\": 20,\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"job_name\": \"sagemaker-pytorch-2021-12-01-04-32-12-691\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-378929394230/sagemaker-pytorch-2021-12-01-04-32-12-691/source/sourcedir.tar.gz\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"model_dir\": \"/opt/ml/model\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":3,\"hidden_dim\":200}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"3\",\"--hidden_dim\",\"200\"]\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-378929394230/sagemaker-pytorch-2021-12-01-04-32-12-691/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":3,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2021-12-01-04-32-12-691\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-378929394230/sagemaker-pytorch-2021-12-01-04-32-12-691/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 3 --hidden_dim 200\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n",
      "\u001b[34mEpoch: 1, BCELoss: 0.6652046685316124\u001b[0m\n",
      "\u001b[34mEpoch: 2, BCELoss: 0.588765947186217\u001b[0m\n",
      "\u001b[34mEpoch: 3, BCELoss: 0.502500096754152\u001b[0m\n",
      "\u001b[34m2021-12-01 04:59:01,083 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-12-01 04:59:54 Uploading - Uploading generated training model\n",
      "2021-12-01 04:59:54 Completed - Training job completed\n",
      "Training seconds: 1535\n",
      "Billable seconds: 1535\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 5: Implantando o modelo em um aplicativo da web\n",
    "\n",
    "Agora que sabemos que nosso modelo está funcionando, é hora de criar algum código de inferência personalizado para que possamos enviar ao modelo uma revisão que não foi processada e determinar o sentimento da revisão.\n",
    "\n",
    "Como vimos acima, por padrão, o estimador que criamos, quando implantado, usará o script de entrada e o diretório que fornecemos ao criar o modelo. No entanto, como agora desejamos aceitar uma string como entrada e nosso modelo espera uma revisão processada, precisamos escrever algum código de inferência personalizado.\n",
    "\n",
    "Vamos armazenar o código que escrevemos no diretório `serve`. Fornecido neste diretório está o arquivo `model.py` que usamos para construir nosso modelo, um arquivo` utils.py` que contém as funções de pré-processamento `review_to_words` e` convert_and_pad` que usamos durante o processamento inicial de dados, e `Predict.py`, o arquivo que conterá nosso código de inferência personalizado. Observe também que `requirements.txt` está presente, o que dirá ao SageMaker quais bibliotecas Python são exigidas por nosso código de inferência personalizado.\n",
    "\n",
    "Ao implantar um modelo PyTorch no SageMaker, espera-se que você forneça quatro funções que o contêiner de inferência SageMaker usará.\n",
    " - `model_fn`: Esta função é a mesma função que usamos no script de treinamento e diz ao SageMaker como carregar nosso modelo.\n",
    " - `input_fn`: esta função recebe a entrada serializada bruta que foi enviada para o endpoint do modelo e seu trabalho é desserializar e disponibilizar a entrada para o código de inferência.\n",
    " - `output_fn`: esta função pega a saída do código de inferência e seu trabalho é serializar esta saída e retorná-la ao chamador do endpoint do modelo.\n",
    " - `Predict_fn`: O coração do script de inferência, é aqui que a previsão real é feita e é a função que você precisa completar.\n",
    "\n",
    "Para o site simples que estamos construindo durante este projeto, os métodos `input_fn` e` output_fn` são relativamente diretos. Só exigimos ser capazes de aceitar uma string como entrada e esperamos retornar um único valor como saída. Você pode imaginar, entretanto, que em um aplicativo mais complexo, a entrada ou saída podem ser dados de imagem ou alguns outros dados binários que exigiriam algum esforço para serializar.\n",
    "\n",
    "\n",
    "### Escrevendo código de inferência\n",
    "\n",
    "Começaremos dando uma olhada no código que foi fornecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LSTMClassifier\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m review_to_words, convert_and_pad\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
      "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\r\n",
      "    model_info = {}\r\n",
      "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model_info = torch.load(f)\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\r\n",
      "\r\n",
      "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model = LSTMClassifier(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[37m# Load the store model parameters.\u001b[39;49;00m\r\n",
      "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.load_state_dict(torch.load(f))\r\n",
      "\r\n",
      "    \u001b[37m# Load the saved word_dict.\u001b[39;49;00m\r\n",
      "    word_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mword_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(word_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model.word_dict = pickle.load(f)\r\n",
      "\r\n",
      "    model.to(device).eval()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m data\r\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInferring sentiment of input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[34mif\u001b[39;49;00m model.word_dict \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no word_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m# TODO: Process input_data so that it is ready to be sent to our model.\u001b[39;49;00m\r\n",
      "    \u001b[37m#       You should produce two variables:\u001b[39;49;00m\r\n",
      "    \u001b[37m#         data_X   - A sequence of length 500 which represents the converted review\u001b[39;49;00m\r\n",
      "    \u001b[37m#         data_len - The length of the review\u001b[39;49;00m\r\n",
      "    \r\n",
      "    data_X_words = review_to_words(input_data)\r\n",
      "    data_X ,data_len = convert_and_pad(model.word_dict, data_X_words)\r\n",
      "\r\n",
      "    \u001b[37m# Using data_X and data_len we construct an appropriate input tensor. Remember\u001b[39;49;00m\r\n",
      "    \u001b[37m# that our model expects input data of the form 'len, review[500]'.\u001b[39;49;00m\r\n",
      "    data_pack = np.hstack((data_len, data_X))\r\n",
      "    data_pack = data_pack.reshape(\u001b[34m1\u001b[39;49;00m, -\u001b[34m1\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    data = torch.from_numpy(data_pack)\r\n",
      "    data = data.to(device)\r\n",
      "\r\n",
      "    \u001b[37m# Make sure to put the model into evaluation mode\u001b[39;49;00m\r\n",
      "    model.eval()\r\n",
      "\r\n",
      "    \u001b[37m# TODO: Compute the result of applying the model to the input data. The variable `result` should\u001b[39;49;00m\r\n",
      "    \u001b[37m#       be a numpy array which contains a single integer which is either 1 or 0\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\r\n",
      "        output = model.forward(data)\r\n",
      "    result = np.round(output.numpy())\r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m result\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize serve/predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como mencionado anteriormente, o método `model_fn` é o mesmo fornecido no código de treinamento e os métodos` input_fn` e `output_fn` são muito simples e sua tarefa será completar o método` predict_fn`. Certifique-se de salvar o arquivo completo como `predict.py` no diretório` serve`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implantando o modelo\n",
    "\n",
    "Agora que o código de inferência personalizado foi escrito, criaremos e implantaremos nosso modelo. Para começar, precisamos construir um novo objeto PyTorchModel que aponta para os artefatos do modelo criados durante o treinamento e também aponta para o código de inferência que desejamos usar. Em seguida, podemos chamar o método de implantação para iniciar o contêiner de implantação.\n",
    "\n",
    "** NOTA **: O comportamento padrão para um modelo PyTorch implantado é assumir que qualquer entrada passada ao preditor é um array `numpy`. Em nosso caso, queremos enviar uma string, então precisamos construir um wrapper simples em torno da classe `RealTimePredictor` para acomodar strings simples. Em uma situação mais complicada, você pode desejar fornecer um objeto de serialização, por exemplo, se desejar enviar dados de imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "\n",
    "model = PyTorchModel(model_data=estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='serve',\n",
    "                     predictor_cls=StringPredictor)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o modelo\n",
    "\n",
    "Agora que implantamos nosso modelo com o código de inferência personalizado, devemos testar para ver se tudo está funcionando. Aqui, testamos nosso modelo carregando os primeiros `250` comentários positivos e negativos e os enviamos para o endpoint, em seguida, coletamos os resultados. O motivo para enviar apenas alguns dos dados é que o tempo que leva para nosso modelo processar a entrada e, em seguida, realizar a inferência é muito longo e, portanto, testar todo o conjunto de dados seria proibitivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def test_reviews(data_dir='../data/aclImdb', stop=250):\n",
    "    \n",
    "    results = []\n",
    "    ground = []\n",
    "    \n",
    "    # We make sure to test both positive and negative reviews    \n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        \n",
    "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n",
    "        files = glob.glob(path)\n",
    "        \n",
    "        files_read = 0\n",
    "        \n",
    "        print('Starting ', sentiment, ' files')\n",
    "        \n",
    "        # Iterate through the files and send them to the predictor\n",
    "        for f in files:\n",
    "            with open(f) as review:\n",
    "                # First, we store the ground truth (was the review positive or negative)\n",
    "                if sentiment == 'pos':\n",
    "                    ground.append(1)\n",
    "                else:\n",
    "                    ground.append(0)\n",
    "                # Read in the review and convert to 'utf-8' for transmission via HTTP\n",
    "                review_input = review.read().encode('utf-8')\n",
    "                # Send the review to the predictor and store the results\n",
    "                results.append(float(predictor.predict(review_input)))\n",
    "                \n",
    "            # Sending reviews to our endpoint one at a time takes a while so we\n",
    "            # only send a small number of reviews\n",
    "            files_read += 1\n",
    "            if files_read == stop:\n",
    "                break\n",
    "            \n",
    "    return ground, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting  pos  files\n",
      "Starting  neg  files\n"
     ]
    }
   ],
   "source": [
    "ground, results = test_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.772"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ground, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que sabemos que nosso endpoint está funcionando conforme o esperado, podemos configurar a página da web que irá interagir com ele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 6: Criando nossa função Lambda\n",
    "\n",
    "Construimos nosso endpoint e deployamos nosso modelo, mas... e se quisessemos que nosso modelo fosse acessável por meio de um webapp? Para isso, precisamos construir alguns outros componentes, que podemos ver na arquitetura abaixo:\n",
    "\n",
    "<img src=\"../deploy-aws.png\">\n",
    "\n",
    "Indo da esquerda para direita temos: \n",
    "\n",
    "- A EC2 será responsável por subir nossa aplicação Flask, servir nossa página e enviar a requisição para o API Gateway;\n",
    "- O API Gateway receberá a requisição e encaminhará para a função Lambda;\n",
    "- Já a função Lambda funcionará como redirecionando o texto da nova avaliação para o Sagemaker endpoint, uma vez que não há como realizar a integração API Gateway -> Sagemaker Endpoint de forma nativa e também não podemos chamar a URL do Sagemaker Endpoint sem estar dentro da AWS.\n",
    "\n",
    "### Configurando nossa função Lambda\n",
    "\n",
    "A primeira coisa que faremos será a criação da nossa função Lambda. Nossa função receberá como input os dados vindos do nosso API Gateway, irá realizar a chamada do endpoint e retornará a resposta para o API Gateway.\n",
    "\n",
    "#### Parte A: Criando IAM Role para a Lambda\n",
    "\n",
    "Já que queremos que nossa função Lambda chame o Sagemaker Endpoint, precisamos garantir que ele tenha permissão para isso. Dessa forma, precisamos adicionar essa permissão dentro da Role que usaremos em nossa função Lambda.\n",
    "\n",
    "Usando o Console da AWS, procuraremos por **IAM** na barra de busca e clicaremos em **Roles** no menu esquerdo. Feito isso, clique em **Create Role**. Garante que em **AWS service** o time de _trusted entity_ selecionado seja **Lambda** e, em seguida, clique em **Next: Permissions**.\n",
    "\n",
    "Na barra de busca procure por `sagemaker` e clique no checkbox referente a **AmazonSageMakerFullAccess** policy. Clique em **Next: Review**.\n",
    "\n",
    "Por último, dê um nome para usa role e garanta que você irá se lembrar na hora de criar sua função Lambda!\n",
    "\n",
    "Usaremos o nome `LambdaSageMakerRole`.\n",
    "\n",
    "#### Parte B: Criando a Lambda\n",
    "\n",
    "Agora, é hora de criarmos nossa função Lambda!\n",
    "\n",
    "Para isso, no console AWS, procure por Lambda e clique em **Create a function**. Na página seguinte, clique em **Author from scratch**, selecione o runtime como sendo Python, dê um nome para sua função, como por exemplo: `sentiment_analysis_func`. Não esqueça de selecionar a role que criamos!\n",
    "\n",
    "Após isso, clique em **Create Function**.\n",
    "\n",
    "Na próxima página você verá algumas informações sobre sua função Lambda que você acabou de criar. Se você descer um pouco a página, verá um editor de texto onde você pode escrever código que será executado quando sua função for chamada. No nosso projeto, usaremos o código abaixo (basta copiar e colar) e não esqueça de mudar o `EndpointName` para o nome do endpoint que pegaremos na célula abaixo.\n",
    "\n",
    "```python\n",
    "# We need to use the low-level library to interact with SageMaker since the SageMaker API\n",
    "# is not available natively through Lambda.\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
    "    response = runtime.invoke_endpoint(EndpointName = '**ENDPOINT NAME HERE**',    # The name of the endpoint we created\n",
    "                                       ContentType = 'text/plain',                 # The data format that is expected\n",
    "                                       Body = event['body'])                       # The actual review\n",
    "\n",
    "    # The response is an HTTP response whose body contains the result of our inference\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "        'body' : result\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-pytorch-2021-12-01-05-13-42-682'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após alterar o nome do endpoint na sua função Lambda, clique em **Salvar**. Feito, sua função Lambda estará pronto para ser executada!\n",
    "\n",
    "## Passo 7: Criando e configurando o API Gateway\n",
    "\n",
    "Agora, é hora de criarmos nosso API Gateway!\n",
    "\n",
    "Abra o console da AWS e busque por **API Gateway** e, na página inicial, procure por **REST API** (vão ter duas, selecione a que NÃO aparece private) e clique em **Build**. Na tela de configuração, na seção **Create new API** selecione **New API** e defina um nome para seu Gateway e clique em **Create API**.\n",
    "\n",
    "Já dentro da sua API, precisamos criar nosso método POST, que receberá o texto a ser predito, e integrá-lo a função Lambda que criamos. Para isso, na aba **Resources** do menu lateral, clicaremos no botão **Actions** e selecionaremos a opção **Create Method**. Na lista dos nossos recursos aparecerá um dropdown vazio, clique nele e selecione **POST** e depois clique no &#x2611;.\n",
    "\n",
    "Na tela de configuração do nosso método POST preencha da seguinte forma:\n",
    "\n",
    "- **Integration type**:  Lambda Function;\n",
    "- **Use Lambda Proxy integration**: marque essa checkbox;\n",
    "- **Lambda Region**: Região que você criou seus outros recursos (no geral, já vem preenchido corretamente);\n",
    "- **Lambda Function**: Nome da função Lambda que criamos no passo anterior.\n",
    "\n",
    "Após preencher, clique em **Save**.\n",
    "\n",
    "Para finalizar, precisamos deployar nosso API Gateway. Para isso, na aba **Resources** do menu lateral, clicamos no botão **Actions** e depois em **Deploy API**. Irá aparecer um pop-up no qual clicaremos em **Deployment stage**, selecionaremos a opção **[New Stage]** e depois daremos um nome a esse estágio (pode ser _prod_). Por fim, clicamos em **Deploy**.\n",
    "\n",
    "Nessa janela que abrirá, você verá na parte superior a URL do seu API Gateway. Salve ela em algum lugar pois usaremos no passo a seguir, quando estivermos configurando nosso webapp!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Deployando nosso web app\n",
    "\n",
    "Agora, como passo final, faremos o deploy do nosso web app, para que possamos acessá-lo pelo browser e saber o sentimento de novas avaliações de filmes!  \n",
    "Para isso, usaremos uma instância de EC2 e subiremos uma aplicação Flask que será responsável por carregar a página e fazer as chamadas no API Gateway.\n",
    "\n",
    "Comece procurando por **EC2** na barra de busca do console AWS. No menu lateral esquerdo clique em **Instances** e procure por um botão laranja, na parte superior direita, com o seguinte texto: **Launch instances**. Clique nele.\n",
    "\n",
    "Após isso, você será redirecionado para uma tela onde terá que selecionar a AMI (imagem do sistema) que você irá utilizar em sua EC2. Selecione **Amazon Linux 2 AMI (HVM), SSD Volume Type** na versão **64-bit (x86)**. Ao clicar em **Select** você será redirecionado para a página onde poderá escolher o tipo da sua instância. Selecioner o tipo **t2.micro** e clique em **Next: Configure Instance Details**.\n",
    "\n",
    "Na página seguinte, não precisa alterar nada, apenas clicar em **Next: Add Storage**. Em seguida, clique em **Next: Add tags** e depois em **Next: Configure Security Group**.\n",
    "\n",
    "Nessa página configurar o `security group` da EC2 para garantir que seja permitido o ingresso pela porta 80 (Adicionar uma inbound rule permitindo HTTP para qualquer Ipv4). Para isso, clicamos em **Add rule**, mudamos o tipo para **HTTP** e o Source para **Anywhere**. Feito isso, clique em **Review and Launch** e depois em **Launch**.\n",
    "\n",
    "Nessa hora, aparecerá um pop-up perdindo para você selecionar um par de chaves para fazer a conexão SSH. Não usaremos, pois iremos nos conectar à máquina via browser. Assim, selecione a opção **Proceed without a key pair**, marque o checkbox e clique em **Launch instances**.\n",
    "\n",
    "Agora, temos que aguardar a máquina ficar com o status de pronta e, quando estiver tudo certo, clicamos em cima da nossa instância e depois em **Connect**.\n",
    "\n",
    "Após isso, vamos acessar, via browser, o terminal da máquina que criamos e instalar o git (```yum install git```). Com o git instalado, iremos executar o comando ```https://github.com/vfcarida/NLP_TDC2021```. Não esqueça de, após clonar, entrar no diretório (```NLP-producao```).\n",
    "\n",
    "Feito isso, iremos acessar a pasta `website\\templates` e abrir o arquivo chamado `index.html` (```vim index.html```) para que possamos alterar a URL da nossa API. Procure por uma linha que contém **\\*\\*REPLACE WITH PUBLIC API URL\\*\\*** e insira a url da sua API onde será possível realizar a predição. Feito isso, salve o arquivo (```Esc -> :wq```) e volte para o diretório `website` (```cd ..```)`.\n",
    "\n",
    "Já no repositório `website`, execute o comando ```pip3 install -r requirements.txt``` e, depois disso, o comando ```python3 -m flask run --host 0.0.0.0 --port 80```.\n",
    "\n",
    "Pronto! Basta acessar o Ip público da sua máquina (fica na parte inferior da tela) e ver sua aplicação funcionando!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passo 9: Deletando a EC2 e o Sagemaker Endpoint\n",
    "\n",
    "Não se esqueça de deletar os recursos assim que finalizar o uso, para que não gere cobranças desnecessárias.\n",
    "\n",
    "Para apagar a EC2 procure por **EC2** na barra de busca do console AWS. No menu lateral esquerdo clique em **Instances** e depois selecione a instância que você criou para o nosso exemplo. Na parte superior, clique em **Instance State** e depois em **Terminate Instance**.\n",
    "\n",
    "Já para o Sagemaker Endpoint, apenas rode o comando da célula abaixo.\n",
    "\n",
    "OBS: Para deletar os outros recursos que criamos (Lambda e API Gateway) basta acessar tais produtos, selecionar o que você quer deletar e clicar em **Actions** e **Delete**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
